{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b96324",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-10T22:49:46.730872Z",
     "iopub.status.busy": "2025-04-10T22:49:46.730470Z",
     "iopub.status.idle": "2025-04-10T22:50:02.174586Z",
     "shell.execute_reply": "2025-04-10T22:50:02.173437Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 15.45159,
     "end_time": "2025-04-10T22:50:02.176608",
     "exception": false,
     "start_time": "2025-04-10T22:49:46.725018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dagshub\r\n",
      "  Downloading dagshub-0.5.9-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting mlflow\r\n",
      "  Downloading mlflow-2.21.3-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\r\n",
      "Collecting appdirs>=1.4.4 (from dagshub)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.1.8)\r\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\r\n",
      "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\r\n",
      "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (14.0.0)\r\n",
      "Collecting dacite~=1.6.0 (from dagshub)\r\n",
      "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.0.0)\r\n",
      "Collecting gql[requests] (from dagshub)\r\n",
      "  Downloading gql-3.5.2-py2.py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.3)\r\n",
      "Collecting treelib>=1.6.4 (from dagshub)\r\n",
      "  Downloading treelib-1.7.1-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting pathvalidate>=3.0.0 (from dagshub)\r\n",
      "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\r\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.37.29)\r\n",
      "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\r\n",
      "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\r\n",
      "  Downloading dagshub_annotation_converter-0.1.8-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting mlflow-skinny==2.21.3 (from mlflow)\r\n",
      "  Downloading mlflow_skinny-2.21.3-py3-none-any.whl.metadata (31 kB)\r\n",
      "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\r\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\r\n",
      "Collecting graphene<4 (from mlflow)\r\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting gunicorn<24 (from mlflow)\r\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\r\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.5)\r\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\r\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\r\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (5.5.2)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (3.1.1)\r\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.21.3->mlflow)\r\n",
      "  Downloading databricks_sdk-0.49.0-py3-none-any.whl.metadata (38 kB)\r\n",
      "Collecting fastapi<1 (from mlflow-skinny==2.21.3->mlflow)\r\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (8.6.1)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (1.16.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (1.16.0)\r\n",
      "Requirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (24.2)\r\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (3.20.3)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (2.11.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (2.32.3)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (0.5.3)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.21.3->mlflow) (4.13.1)\r\n",
      "Collecting uvicorn<1 (from mlflow-skinny==2.21.3->mlflow)\r\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.3.1)\r\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.1.0)\r\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\r\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\r\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\r\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\r\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.29 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.37.29)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.11.4)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\r\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\r\n",
      "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.19.0)\r\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\r\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\r\n",
      "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (2.27.0)\r\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.21.3->mlflow)\r\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.21.3->mlflow) (3.21.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (1.2.18)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (75.1.0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (0.37b0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (2.33.1)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.21.3->mlflow) (0.4.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.21.3->mlflow) (3.4.1)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\r\n",
      "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.2.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.21.3->mlflow) (1.17.2)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (0.4.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (4.9)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.21.3->mlflow) (0.6.1)\r\n",
      "Downloading dagshub-0.5.9-py3-none-any.whl (260 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mlflow-2.21.3-py3-none-any.whl (28.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mlflow_skinny-2.21.3-py3-none-any.whl (6.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\r\n",
      "Downloading dagshub_annotation_converter-0.1.8-py3-none-any.whl (33 kB)\r\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\r\n",
      "Downloading treelib-1.7.1-py3-none-any.whl (19 kB)\r\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading databricks_sdk-0.49.0-py3-none-any.whl (683 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m684.0/684.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gql-3.5.2-py2.py3-none-any.whl (74 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: appdirs, uvicorn, treelib, pathvalidate, gunicorn, graphql-core, dacite, backoff, starlette, graphql-relay, gql, graphene, fastapi, databricks-sdk, mlflow-skinny, dagshub-annotation-converter, mlflow, dagshub\r\n",
      "  Attempting uninstall: dacite\r\n",
      "    Found existing installation: dacite 1.9.2\r\n",
      "    Uninstalling dacite-1.9.2:\r\n",
      "      Successfully uninstalled dacite-1.9.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 backoff-2.2.1 dacite-1.6.0 dagshub-0.5.9 dagshub-annotation-converter-0.1.8 databricks-sdk-0.49.0 fastapi-0.115.12 gql-3.5.2 graphene-3.4.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.21.3 mlflow-skinny-2.21.3 pathvalidate-3.2.3 starlette-0.46.1 treelib-1.7.1 uvicorn-0.34.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dagshub mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9060d128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:02.192805Z",
     "iopub.status.busy": "2025-04-10T22:50:02.192472Z",
     "iopub.status.idle": "2025-04-10T22:50:40.302032Z",
     "shell.execute_reply": "2025-04-10T22:50:40.301109Z"
    },
    "papermill": {
     "duration": 38.119508,
     "end_time": "2025-04-10T22:50:40.303933",
     "exception": false,
     "start_time": "2025-04-10T22:50:02.184425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=e408d233-596c-4b2b-97fd-7703fb4ee1d1&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=41d044a92283a92791d4ba34c34cca5470f59adb7b2d3a42734ea13764024ab9\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba1172fd42545268d5faddc608bdc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as zeliz22\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as zeliz22\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"zeliz22/ML_House-Pricing\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"zeliz22/ML_House-Pricing\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository zeliz22/ML_House-Pricing initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository zeliz22/ML_House-Pricing initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='zeliz22', repo_name='ML_House-Pricing', mlflow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2a3035",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:41.438572Z",
     "iopub.status.busy": "2025-04-10T22:50:41.438327Z",
     "iopub.status.idle": "2025-04-10T22:50:43.580118Z",
     "shell.execute_reply": "2025-04-10T22:50:43.578768Z"
    },
    "papermill": {
     "duration": 3.266597,
     "end_time": "2025-04-10T22:50:43.581732",
     "exception": false,
     "start_time": "2025-04-10T22:50:40.315135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n",
      "/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec69904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:43.600621Z",
     "iopub.status.busy": "2025-04-10T22:50:43.600021Z",
     "iopub.status.idle": "2025-04-10T22:50:43.652549Z",
     "shell.execute_reply": "2025-04-10T22:50:43.651263Z"
    },
    "papermill": {
     "duration": 0.064291,
     "end_time": "2025-04-10T22:50:43.654544",
     "exception": false,
     "start_time": "2025-04-10T22:50:43.590253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df =  pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c57a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:43.671665Z",
     "iopub.status.busy": "2025-04-10T22:50:43.671355Z",
     "iopub.status.idle": "2025-04-10T22:50:45.521552Z",
     "shell.execute_reply": "2025-04-10T22:50:45.520507Z"
    },
    "papermill": {
     "duration": 1.860925,
     "end_time": "2025-04-10T22:50:45.523520",
     "exception": false,
     "start_time": "2025-04-10T22:50:43.662595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['SalePrice'])\n",
    "y = df['SalePrice']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa10a0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:45.541353Z",
     "iopub.status.busy": "2025-04-10T22:50:45.540882Z",
     "iopub.status.idle": "2025-04-10T22:50:45.563242Z",
     "shell.execute_reply": "2025-04-10T22:50:45.562246Z"
    },
    "papermill": {
     "duration": 0.033509,
     "end_time": "2025-04-10T22:50:45.564941",
     "exception": false,
     "start_time": "2025-04-10T22:50:45.531432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_with_nulls = list(col for col in X_train.columns if X_train[col].isna().sum() >0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004d9f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:45.581944Z",
     "iopub.status.busy": "2025-04-10T22:50:45.581534Z",
     "iopub.status.idle": "2025-04-10T22:50:45.598028Z",
     "shell.execute_reply": "2025-04-10T22:50:45.597075Z"
    },
    "papermill": {
     "duration": 0.026887,
     "end_time": "2025-04-10T22:50:45.599614",
     "exception": false,
     "start_time": "2025-04-10T22:50:45.572727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Union\n",
    "\n",
    "class DataCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 numeric_strategy: str = 'mean',\n",
    "                 categorical_strategy: str = 'most_frequent',\n",
    "                 numeric_fill_value: Optional[Union[int, float]] = None,\n",
    "                 categorical_fill_value: Optional[str] = None,\n",
    "                 drop_threshold: float = 0.8):\n",
    "\n",
    "        self.numeric_strategy = numeric_strategy\n",
    "        self.categorical_strategy = categorical_strategy\n",
    "        self.numeric_fill_value = numeric_fill_value\n",
    "        self.categorical_fill_value = categorical_fill_value\n",
    "        self.drop_threshold = drop_threshold\n",
    "        self.numeric_impute_values_ = {}\n",
    "        self.categorical_impute_values_ = {}\n",
    "        self.columns_to_drop_ = []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"Learn imputation values from the data\"\"\"\n",
    "        \n",
    "        # Identify columns to drop\n",
    "        null_ratios = X.isnull().mean()\n",
    "        self.columns_to_drop_ = list(null_ratios[null_ratios > self.drop_threshold].index)\n",
    "        X_clean = X.drop(columns=self.columns_to_drop_)\n",
    "        \n",
    "        # Separate numeric and categorical columns\n",
    "        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n",
    "        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n",
    "        \n",
    "        # Calculate numeric imputation values\n",
    "        for col in numeric_cols:\n",
    "            if self.numeric_strategy == 'mean':\n",
    "                self.numeric_impute_values_[col] = X_clean[col].mean()\n",
    "            elif self.numeric_strategy == 'median':\n",
    "                self.numeric_impute_values_[col] = X_clean[col].median()\n",
    "            elif self.numeric_strategy == 'constant':\n",
    "                if self.numeric_fill_value is None:\n",
    "                    raise ValueError(\"numeric_fill_value must be specified for constant strategy\")\n",
    "                self.numeric_impute_values_[col] = self.numeric_fill_value\n",
    "            elif self.numeric_strategy != 'drop':\n",
    "                raise ValueError(f\"Unknown numeric strategy: {self.numeric_strategy}\")\n",
    "        \n",
    "        # Calculate categorical imputation values\n",
    "        for col in categorical_cols:\n",
    "            if self.categorical_strategy == 'most_frequent':\n",
    "                self.categorical_impute_values_[col] = X_clean[col].mode()[0]\n",
    "            elif self.categorical_strategy == 'constant':\n",
    "                if self.categorical_fill_value is None:\n",
    "                    raise ValueError(\"categorical_fill_value must be specified for constant strategy\")\n",
    "                self.categorical_impute_values_[col] = self.categorical_fill_value\n",
    "            elif self.categorical_strategy != 'drop':\n",
    "                raise ValueError(f\"Unknown categorical strategy: {self.categorical_strategy}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply the learned imputation to new data\"\"\"\n",
    "        \n",
    "        # Drop high-null columns\n",
    "        X_clean = X.drop(columns=self.columns_to_drop_)\n",
    "        \n",
    "        # Separate numeric and categorical columns\n",
    "        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n",
    "        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n",
    "        \n",
    "        # Apply numeric imputation\n",
    "        for col in numeric_cols:\n",
    "            if col in self.numeric_impute_values_:\n",
    "                X_clean[col] = X_clean[col].fillna(self.numeric_impute_values_[col])\n",
    "            elif self.numeric_strategy == 'drop':\n",
    "                X_clean = X_clean.dropna(subset=[col])\n",
    "        \n",
    "        # Apply categorical imputation\n",
    "        for col in categorical_cols:\n",
    "            if col in self.categorical_impute_values_:\n",
    "                X_clean[col] = X_clean[col].fillna(self.categorical_impute_values_[col])\n",
    "            elif self.categorical_strategy == 'drop':\n",
    "                X_clean = X_clean.dropna(subset=[col])\n",
    "        \n",
    "        return X_clean\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612838a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:45.616715Z",
     "iopub.status.busy": "2025-04-10T22:50:45.616420Z",
     "iopub.status.idle": "2025-04-10T22:50:45.626718Z",
     "shell.execute_reply": "2025-04-10T22:50:45.625542Z"
    },
    "papermill": {
     "duration": 0.020542,
     "end_time": "2025-04-10T22:50:45.628400",
     "exception": false,
     "start_time": "2025-04-10T22:50:45.607858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "class CustomEncoder:\n",
    "    def __init__(self, threshold = 3):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Initialize encoders\n",
    "        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        \n",
    "        # Store feature names for one-hot encoding\n",
    "        self.one_hot_feature_names = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "        s = X[cat_cols].nunique()\n",
    "\n",
    "        self.ordinal_cols = list(s[s > self.threshold].index)\n",
    "        self.one_hot_cols = list(s[s <= self.threshold].index)\n",
    "\n",
    "        if self.one_hot_cols:\n",
    "            self.one_hot_encoder.fit(X[self.one_hot_cols])\n",
    "            self.one_hot_feature_names = self.one_hot_encoder.get_feature_names_out(self.one_hot_cols)\n",
    "        \n",
    "        if self.ordinal_cols:\n",
    "            self.ordinal_encoder.fit(X[self.ordinal_cols])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Apply One-Hot Encoding\n",
    "        if self.one_hot_cols:\n",
    "            one_hot_encoded = self.one_hot_encoder.transform(X[self.one_hot_cols])\n",
    "            one_hot_df = pd.DataFrame(one_hot_encoded, columns=self.one_hot_feature_names, index=X.index)\n",
    "            X_transformed = pd.concat([X_transformed, one_hot_df], axis=1)\n",
    "            X_transformed.drop(self.one_hot_cols, axis=1, inplace=True)\n",
    "        \n",
    "        # Apply Ordinal Encoding\n",
    "        if self.ordinal_cols:\n",
    "            ordinal_encoded = self.ordinal_encoder.transform(X[self.ordinal_cols])\n",
    "            ordinal_df = pd.DataFrame(ordinal_encoded, columns=self.ordinal_cols, index=X.index)\n",
    "            X_transformed[self.ordinal_cols] = ordinal_df\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X, y = None):\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e110c0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:45.645853Z",
     "iopub.status.busy": "2025-04-10T22:50:45.645544Z",
     "iopub.status.idle": "2025-04-10T22:50:45.654508Z",
     "shell.execute_reply": "2025-04-10T22:50:45.653195Z"
    },
    "papermill": {
     "duration": 0.019833,
     "end_time": "2025-04-10T22:50:45.656282",
     "exception": false,
     "start_time": "2025-04-10T22:50:45.636449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CorrelationFeatureDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0.8):\n",
    "        self.threshold = threshold\n",
    "        self.features_to_drop = []\n",
    "        self.high_corr_pairs = []\n",
    "        \n",
    "    def fit(self, X, y): \n",
    "        X_corr = X.copy()\n",
    "        X_corr['SalesPrice'] = y\n",
    "        corr_matrix = X_corr.corr().abs()\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                \n",
    "                if corr_matrix.iloc[i, j] > self.threshold:\n",
    "                    self.high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "                    \n",
    "        for feat1, feat2, _ in self.high_corr_pairs:\n",
    "            if abs(X[feat1].corr(y)) < abs(X[feat2].corr(y)):\n",
    "                self.features_to_drop.append(feat1)\n",
    "            else:\n",
    "                self.features_to_drop.append(feat2)\n",
    "        \n",
    "        self.features_to_drop = list(set(self.features_to_drop))\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "      return X.drop(columns=self.features_to_drop)\n",
    "        \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb87ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:45.673899Z",
     "iopub.status.busy": "2025-04-10T22:50:45.673560Z",
     "iopub.status.idle": "2025-04-10T22:50:49.878627Z",
     "shell.execute_reply": "2025-04-10T22:50:49.877116Z"
    },
    "papermill": {
     "duration": 4.216102,
     "end_time": "2025-04-10T22:50:49.880352",
     "exception": false,
     "start_time": "2025-04-10T22:50:45.664250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13/1305265050.py\", line 12, in log_rmse\n",
      "  File \"/tmp/ipykernel_13/1305265050.py\", line 10, in rmse\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13/1305265050.py\", line 12, in log_rmse\n",
      "  File \"/tmp/ipykernel_13/1305265050.py\", line 10, in rmse\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan -0.17365644         nan -0.17365644]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Log RMSE: 0.1492\n",
      "\n",
      "Test Log RMSE: 0.1859\n",
      "Best parameters: {'scaler': RobustScaler()}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "# 1. Define evaluation metrics\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "def log_rmse(y_true, y_pred):\n",
    "    return rmse(np.log1p(y_true), np.log1p(y_pred))\n",
    "# 2. Create custom scorer\n",
    "log_rmse_scorer = make_scorer(log_rmse, greater_is_better=False)\n",
    "# 3. Set up K-Fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# 4. Define pipeline\n",
    "pipeline_linear = Pipeline([\n",
    "    ('cleaner', DataCleaner(numeric_strategy = 'median')),\n",
    "    ('encoder', CustomEncoder(threshold=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "# 5. Define parameter grid\n",
    "param_grid = {\n",
    "    'scaler': [StandardScaler(), RobustScaler(), MinMaxScaler(), None]\n",
    "}\n",
    "# 6. Set up GridSearchCV with K-Fold\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_linear,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,  # Using our K-Fold here\n",
    "    scoring=log_rmse_scorer,\n",
    "    refit=True,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "# 7. Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "# Training set evaluation\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_log_rmse = log_rmse(y_train, train_preds)\n",
    "print(f\"\\nTraining Log RMSE: {train_log_rmse:.4f}\")\n",
    "# Test set evaluation\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_log_rmse = log_rmse(y_test, test_preds)\n",
    "print(f\"\\nTest Log RMSE: {test_log_rmse:.4f}\")\n",
    "best_param = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ab1ac63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:50:49.900890Z",
     "iopub.status.busy": "2025-04-10T22:50:49.899943Z",
     "iopub.status.idle": "2025-04-10T22:51:08.807639Z",
     "shell.execute_reply": "2025-04-10T22:51:08.806490Z"
    },
    "papermill": {
     "duration": 18.918242,
     "end_time": "2025-04-10T22:51:08.809031",
     "exception": false,
     "start_time": "2025-04-10T22:50:49.890789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/10 22:51:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LinearRegression using median for numericals at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/4/runs/7bd48323f83c44339aeb8ddedf2e1b25\n",
      "🧪 View experiment at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/4\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = 'Linear Model'\n",
    "run_name = 'LinearRegression using median for numericals'\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.log_params(best_param)\n",
    "    mlflow.log_metric(\"train_log_rmse\", train_log_rmse)\n",
    "    mlflow.log_metric(\"test_log_rmse\", test_log_rmse)\n",
    "    mlflow.sklearn.log_model(best_model, \"linear_model\")\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"cv_strategy\", \"KFold-5\")\n",
    "    mlflow.log_param(\"param_grid\", str(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae2b1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:51:08.827784Z",
     "iopub.status.busy": "2025-04-10T22:51:08.827241Z",
     "iopub.status.idle": "2025-04-10T22:53:11.169944Z",
     "shell.execute_reply": "2025-04-10T22:53:11.168753Z"
    },
    "papermill": {
     "duration": 122.35421,
     "end_time": "2025-04-10T22:53:11.171803",
     "exception": false,
     "start_time": "2025-04-10T22:51:08.817593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized grid search...\n",
      "[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n",
      "[CV] END ............................scaler=StandardScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=RobustScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END ........................................scaler=None; total time=   0.3s\n",
      "[CV] END ............................scaler=StandardScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=RobustScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=RobustScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END ........................................scaler=None; total time=   0.3s\n",
      "[CV] END ............................scaler=StandardScaler(); total time=   0.3s\n",
      "[CV] END ..............................scaler=RobustScaler(); total time=   0.3s\n",
      "[CV] END ..............................scaler=MinMaxScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END ........................................scaler=None; total time=   0.3s\n",
      "[CV] END ............................scaler=StandardScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=RobustScaler(); total time=   0.2s\n",
      "[CV] END ..............................scaler=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END ........................................scaler=None; total time=   0.3s\n",
      "[CV] END ........................................scaler=None; total time=   0.2s\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Grid search completed!\n",
      "\n",
      "Training Log RMSE: 0.0833\n",
      "Test Log RMSE: 0.1634\n",
      "Best parameters: {'model__max_depth': 10, 'model__max_features': 'sqrt', 'model__n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 1. Define evaluation metrics\n",
    "def log_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "\n",
    "# 2. Create custom scorer\n",
    "log_rmse_scorer = make_scorer(log_rmse, greater_is_better=False)\n",
    "\n",
    "# 3. Set up K-Fold cross-validation\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. Updated pipeline using RandomForest\n",
    "pipeline_random_forest = Pipeline([\n",
    "    ('cleaner', DataCleaner()),\n",
    "    ('encoder', CustomEncoder(threshold=3)),\n",
    "    ('correlation_dropper', CorrelationFeatureDropper(threshold=0.8)),\n",
    "    ('feature_selector', RFE(\n",
    "        estimator=RandomForestRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            n_jobs=2\n",
    "        ),\n",
    "        step=1,\n",
    "        n_features_to_select=15\n",
    "    )),\n",
    "    ('model', RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=2\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5. Corrected parameter grid (using double underscores)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [5, 10],\n",
    "    'model__max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# 6. GridSearch setup\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_random_forest,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=log_rmse_scorer,\n",
    "    refit=True,\n",
    "    verbose=3,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "print(\"Starting optimized grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid search completed!\")\n",
    "\n",
    "# 7. Results output (fixed attribute names)\n",
    "best_model = grid_search.best_estimator_\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_log_rmse = log_rmse(y_train, train_preds)\n",
    "print(f\"\\nTraining Log RMSE: {train_log_rmse:.4f}\")\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_log_rmse = log_rmse(y_test, test_preds)\n",
    "print(f\"Test Log RMSE: {test_log_rmse:.4f}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356437d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:53:11.190245Z",
     "iopub.status.busy": "2025-04-10T22:53:11.189861Z",
     "iopub.status.idle": "2025-04-10T22:53:19.496033Z",
     "shell.execute_reply": "2025-04-10T22:53:19.494906Z"
    },
    "papermill": {
     "duration": 8.317336,
     "end_time": "2025-04-10T22:53:19.497743",
     "exception": false,
     "start_time": "2025-04-10T22:53:11.180407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/10 22:53:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run RandomForestRegressor with both RFE and correlation_dropper at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/5/runs/c387ed3f922f4b5c85ffd68f987e570d\n",
      "🧪 View experiment at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/5\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = 'Random Forest Regressor Model'\n",
    "run_name = 'RandomForestRegressor with both RFE and correlation_dropper'\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.log_params(best_param)\n",
    "    mlflow.log_metric(\"train_log_rmse\", train_log_rmse)\n",
    "    mlflow.log_metric(\"test_log_rmse\", test_log_rmse)\n",
    "    mlflow.sklearn.log_model(best_model, \"linear_model\")\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"cv_strategy\", \"KFold-5\")\n",
    "    mlflow.log_param(\"param_grid\", str(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b886f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:53:19.516113Z",
     "iopub.status.busy": "2025-04-10T22:53:19.515740Z",
     "iopub.status.idle": "2025-04-10T22:53:28.072296Z",
     "shell.execute_reply": "2025-04-10T22:53:28.071266Z"
    },
    "papermill": {
     "duration": 8.567629,
     "end_time": "2025-04-10T22:53:28.074029",
     "exception": false,
     "start_time": "2025-04-10T22:53:19.506400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimized grid search...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Grid search completed!\n",
      "\n",
      "Training Log RMSE: 0.0954\n",
      "Test Log RMSE: 0.1478\n",
      "Best parameters: {'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__n_estimators': 100, 'model__subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# 1. Define evaluation metrics\n",
    "def log_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log1p(y_true), np.log1p(y_pred)))\n",
    "\n",
    "# 2. Create custom scorer\n",
    "log_rmse_scorer = make_scorer(log_rmse, greater_is_better=False)\n",
    "\n",
    "# 3. Set up K-Fold cross-validation (reduced to 3 folds)\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 4. Simplified pipeline\n",
    "pipeline_XGB = Pipeline([\n",
    "    ('cleaner', DataCleaner()),\n",
    "    ('encoder', CustomEncoder(threshold=3)),\n",
    "    ('correlation_dropper', CorrelationFeatureDropper(threshold=0.8)),\n",
    "    ('feature_selector', RFE(\n",
    "        estimator=xgb.XGBRegressor(\n",
    "            n_estimators=30,  # Reduced from 50\n",
    "            max_depth=3,     # Reduced depth\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        ),\n",
    "        step=1,\n",
    "        n_features_to_select=15  # Fixed value for initial run\n",
    "    )),\n",
    "    ('model', xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=2,  # Reduced parallelization\n",
    "        objective='reg:squarederror'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5. Corrected parameter grid (using proper double underscores)\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100],      # Fixed: model__n_estimators\n",
    "    'model__learning_rate': [0.05],    # Fixed: model__learning_rate\n",
    "    'model__max_depth': [5],           # Fixed: model__max_depth\n",
    "    'model__subsample': [0.8]          # Fixed: model__subsample\n",
    "}\n",
    "\n",
    "# 6. Faster GridSearch configuration\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_XGB,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=log_rmse_scorer,\n",
    "    refit=True,\n",
    "    verbose=3,  # More detailed progress\n",
    "    n_jobs=2    # Further reduced parallelization\n",
    ")\n",
    "\n",
    "print(\"Starting optimized grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid search completed!\")\n",
    "\n",
    "# 7. Results output (fixed attribute names)\n",
    "best_model = grid_search.best_estimator_  # Fixed: best_estimator_\n",
    "train_preds = best_model.predict(X_train)\n",
    "train_log_rmse = log_rmse(y_train, train_preds)\n",
    "print(f\"\\nTraining Log RMSE: {train_log_rmse:.4f}\")\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_log_rmse = log_rmse(y_test, test_preds)\n",
    "print(f\"Test Log RMSE: {test_log_rmse:.4f}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")  # Fixed: best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98be04cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T22:53:28.093965Z",
     "iopub.status.busy": "2025-04-10T22:53:28.093590Z",
     "iopub.status.idle": "2025-04-10T22:53:36.322357Z",
     "shell.execute_reply": "2025-04-10T22:53:36.321255Z"
    },
    "papermill": {
     "duration": 8.240633,
     "end_time": "2025-04-10T22:53:36.324061",
     "exception": false,
     "start_time": "2025-04-10T22:53:28.083428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/04/10 22:53:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBRegressor with both: RFE and correlation_dropper at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/6/runs/3020c68a52a74779a84fbf5c044d8a7c\n",
      "🧪 View experiment at: https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/6\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = 'XGBRegressor Model'\n",
    "run_name = 'XGBRegressor with both: RFE and correlation_dropper'\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    mlflow.log_params(best_param)\n",
    "    mlflow.log_metric(\"train_log_rmse\", train_log_rmse)\n",
    "    mlflow.log_metric(\"test_log_rmse\", test_log_rmse)\n",
    "    mlflow.sklearn.log_model(best_model, \"linear_model\")\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"cv_strategy\", \"KFold-5\")\n",
    "    mlflow.log_param(\"param_grid\", str(param_grid))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 237.271876,
   "end_time": "2025-04-10T22:53:38.952882",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-10T22:49:41.681006",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0c31984d5e464ecaa3016fe8182899e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bba1172fd42545268d5faddc608bdc79": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_0c31984d5e464ecaa3016fe8182899e8",
       "msg_id": "",
       "outputs": [
        {
         "data": {
          "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠇</span> Waiting for authorization\n</pre>\n",
          "text/plain": "\u001b[32m⠇\u001b[0m Waiting for authorization\n"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ],
       "tabbable": null,
       "tooltip": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
