# ML_House-Pricing
დავალება: გვაქვს ბაზა სადაც მოცემულია გასაყიდი სახლების მახასიათებლები და მათი გასაყიდი ფასები. ჩვენ გვინდა, რომ ეს მახასიათებლები დავუკავშიროთ ფასებს ისე, რომ შემდეგში თუკი გამოჩნდება ახალი სახლი გასაყიდად, მისი მახასიათებლებით შევძლოთ წინასწარი ვარაუდი და სავარაუდო გასაყიდი ფასის განსაზღვრა.  
ამოცანის გადასაჭრელად, მოცემული სატრენინგო დატა გავყოთ ორ ნაწილად, 80/20-ზე, რათა შევძლოთ მისი დატრენინგება და შემდეგ დატესტვა, რომ შევაფასოთ უკეთესი მოდელი. 
ერთ-ერთი 

# files
model_experiment.ipynb- ძირითადად ვიმუშავე ამ ფაილში. აქაა ის კლასები, რომლებსაც შემდეგ ვიყენებ ფაიფლაინებში. აქაა აწყობილი სხვადასხვა მოდელი, თავისი პარამეტრებით. ყველა გაშვებული ექსპერიმენტი დალოგილია mlFlowზე
model_inference.ipynb- ამ ფაილში mlFlowდან ვალოუდებ საუკეთესო მოდელს, რომლითაც ვაკეთებ ტესტსეტში არსებული სახლების ფასების დაფრედიქთებას. 

# Feature Engineering
Nan მნიშვნელობების შესაცვლელად შევქმენი კლასი, DataCleaner, რომელელიც ნანებს ცვლის იმის მიხედვით კატეგორიალ სვეტია თუ ნუმერიქალ. კატეგორიალის შემთხვევაში დეფოლტ მნიშვნელობა არის მოდით ჩანაცვლება, ნუმერიკალის შემთხვევაში-საშუალოთი. თუმცა ამ სტრატეგიების ხელთ გადაცემაც შესაძლებელია, რასაც გამოვიყენებ კიდეც ექსპერიმენტებში. 
კატეგორიალი ცვლადების რიცხვითში გადასაყვანად გამოვიყენე  ordinary encoding და one-hot-encoding. ეს უკანასკნელი გამოვიყენე მხოლოდ იმ სვეტებში, სადაც უნიკალური ცვლადების რაოდენობა იყო ნაკლები სამზე, რათა სვეტების რაოდენობა ძალიან არ გაზრდილიყო.  

ასევე, ზედმეტი სვეტების გასაქრობად მაქვს კლასი, CorrelationFeatureDropper, რომელიც პოულობს ერთმანეთთან მაღალკორელაციაში მყოფ სვეტებს და შლის მათ შორის იმას, რომელიც უფრო ნაკლებადაა კორელაციაში საბოლოო დასაფრედიქთებელ სვეტთან. 

# training
სატრენინგო-სავალიდაციოდ გამოვიყენე სტანდარტული KFold მეთოდი, ტრეინინგ დატის 5 ნაწილად გაყოფით.

პირველად მოვსინჯე Linear Regression მოდელი, რომელმაც მომცა 0.2იანი ერორი(ლოგარითმებმოდებულზე). ამავე მოდელში ოდნავ გასაუმჯობესებლად Nanების შევსების დეფოლტ სტრატეგია(საშუალო) ჩავანაცვლე მედიანით, რამაც ცვლილება თითქმის არ გამოიწვია, ამიტომ შემდეგ მოდელებში ეს ცვლილება აღარ გამომიყენებია და ყველგან ვიყენებდი დეფოლტ სტრატეგიას. 

შემდეგ შევცვალე მოდელი და ვსინჯე RandomForestRegression. თავდაპირველად გამოვიყენე ზემოთ ნახსენები კლასი, მაღალკორელაციური სვეტების დასადროპად. წრფივთან შედარებით შედეგი გაუმჯობესდა და მივიღე 0.167. თუმცა უფრო გასაუმჯობესებლად ცვადე corellation_dropperის ჩანაცვლება RFEით და ამ ორივეს კომბინაცია. უმნიშვნელოდ უკეTესი შედეგი აჩვენა იმ ექსპერიმენტმა, რომელშიც გამოვიყენე ორივე: corelattion_dropperიც და RFEიც 

მესამე მოდული კი ვცადე XGBRegressor, რომელმაც არსებული მოდელებიდან საუკეთესო შედეგი აჩვენა. აქაც ვცადე შემეფასებინა შედარებითობა, რომელი ჯობდა-კორელაციის გამოყენება თუ RFEის და ამისთვის ჩავატარე რამდენიმე ექსპერიმენტი. აქ, წინა მოდელისგან განსხვავებით, ყველაზე დაბალი ერორი მივიღე მაშინ, როცა ვიყენებდი მხოლოდ corealttion_dropperს, 0.143, რაც გამოვიდა კიდეც ყველა მოდელის საუკეთესო შეფასება. 

# MLflow
Linear regression - (2 run) - https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/4?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D

RandomForestRegression - 3(run) - https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/5?viewStateShareKey=3894e7dac091113a949e1a0b144bdfbf23f857b1cfb2b6251e919052fe25b155&compareRunsMode=TABLE

XGB Regression -(3 run) - https://dagshub.com/zeliz22/ML_House-Pricing.mlflow/#/experiments/6?viewStateShareKey=3894e7dac091113a949e1a0b144bdfbf23f857b1cfb2b6251e919052fe25b155&compareRunsMode=TABLE

მეტრიკები: train_log_rmse - სატრენინგო დატას ერორი დასწავლის შემდეგ(გასაშუალოებული, რადგან ვიყენებ KFoldს)(RMSE ამოღებულია ლოგარითმმოდებული ფასებიდან, როგორც ამას     
           პირობა გვთხოვდა)
           test_log_rmse - ჩემ მეირ გამოყოფილი სატესტო დატას ერორი, ასევე გამოთვლილი RMSE მეთოდით, ლოგარითმმოდებულ ფასებზე. 

 საუკეთესო მოდელის შედეგები: train_log_rmse = 0.07758591347492581
                               test_log_rmse = 0.14358060387087443
                               submission result = 0.14013


